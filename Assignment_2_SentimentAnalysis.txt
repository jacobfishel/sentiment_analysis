
Assignment #2: Sentiment Analysis
Total Score: 130
This assignment can be completed individually or by a team of up to three members.
Due Date: See the course page

Objectives
Build sentiment analysis models and apply them to financial news to estimate the impact of news on stock prices.

Datasets
Two financial news datasets collected between 2009–2020 by publishers are provided:

- analyst_ratings.csv – Headlines collected by Publisher A, which may include price targets estimated by professional analysts through complex modeling to estimate the current intrinsic value of the company.
- headlines.csv – Headlines collected by Publisher B, without price target information.

Schema
Both datasets follow a similar schema:

analyst_ratings(id, headline, URL, publisher, date, symbol)
- id: row number
- headline: headline news that may include the target price (not all headlines indicate the target price)
- URL: the URL of the headline
- publisher: news publisher
- date: published date and time
- symbol: stock symbol representing the company

headlines(id, headline, URL, publisher, date, symbol)

Problem Context
Stock prices fluctuate daily due to company developments, sector trends, market sentiment, and unpredictable external factors. Since a company’s intrinsic value is unknown, predicting stock price movements is extremely challenging.

In this assignment, you will develop sentiment analysis models that estimate the potential impact of financial news on future stock prices and write a brief analysis report on the results.

Organization of Program Files and Directories
The required files and directory structure are provided in Assignment_2.zip.

Before getting started, review the helper functions and test files:
util.py, webscraping.py, webscraping_headless_parsing.py, dataset_schema.py, test_dataset_schema.py

Each file is currently blank and must be completed by you. Do not rename or rearrange directories or files.

Example execution:
python 2_trading_sim_eval.py
should run without extra parameters.

Required Tasks

Phase 1: Data Collection and Preprocessing

1.1 Collect Historical Prices, S&P 500 Indices, and News Data
Complete 1_collect_data.py.

(a) Download historical daily prices
Download daily prices for each stock symbol (and S&P 500) from a free data source (e.g., Yahoo Finance) using yfinance or yahoo-fin.

Save as historical_prices.csv with fields:
date, symbol, open, high, low, close, volume

Use adjusted close for close. Treat the S&P 500 index like a stock with the symbol s&p.

(b) Download full news articles
Use Python tools such as requests, BeautifulSoup, playwright, or selenium.

Ethical and legal guidelines:
- Check robots.txt to ensure scraping is allowed.
- Respect rate limits.
- Use official APIs when possible.

Full news articles can be found from Benzinga, Zacks, Gurufocus, and SeekingAlpha.
If scraping is blocked (e.g., Gurufocus), skip this step.

(c) Merge datasets
Create all_news.csv by merging both datasets and adding a new field article for full articles.

Schema:
all_news(date, symbol, headline, URL, article, publisher)

1.2 Calculate Volatility
Complete 2_calculate_volatility.py.

Assume impactful news affects stock prices for three days.
Compute the average daily volatility as the standard deviation of returns over three trading days.

Daily log return
Using continuous compounding:
r_t = ln(P_t / P_{t-1}) = ln(1 + R_t)
where P_t is price at time t and P_{t-1} is price three trading days earlier.

Daily volatility
σ_w = stdev(r_t)

Annualized volatility:
σ_annual = σ_w * sqrt(252)

Market-adjusted return and volatility
Model:
r_{a,t} = α + β r_{m,t} + ε_t

Where:
- r_{a,t}: observed asset return
- r_{m,t}: market (S&P 500) return
- α: expected return after accounting for market exposure
- β: stock’s sensitivity to the market
- ε_t: idiosyncratic (residual) return

Market-adjusted (idiosyncratic) return:
ε_t = r_{a,t} - (α + β r_{m,t})

Market-adjusted volatility:
σ_ε = sqrt((1/(N-1)) * sum_{t=1}^N (ε_t - mean(ε))^2)

Total asset volatility:
σ_{a,t} = stdev(r_{a,t})

Interpretation
- Beta (β): measures sensitivity to market
  - β = 1 → moves with market
  - β > 1 → more volatile
  - β < 1 → less volatile
  - β < 0 → opposite movement
- Alpha (α): abnormal return after accounting for market exposure
- Residual (εₜ): portion of return not explained by market

1.3 Estimate Impact Scores
Complete 3_estimate_impact_scores.py.

Calculate an impact score in the range [-3, +3]:
impact_score_t = sign(r_t) * f(σ_t)
where:
- negative values = negative impact
- positive values = positive impact
- 0 = neutral

Z-score normalization
z_r = (ε_t - μ_r) / σ_r,  z_σ = (σ_ε - μ_σ) / σ_σ

Composite impact score
impact_score =
0, if |z_r| ≤ 0.5
sign(z_r) * [1 + I(|z_r| > 1) + I(z_σ > 1)], if |z_r| > 0.5
where I(·) = 1 if true, else 0.

Create historical_prices_impact.csv with:
date, symbol, open, high, low, close, volume, daily_return, daily_volatility, market_return, beta, alpha, market_adj_return, market_adj_volatility, impact_score

1.4 Identify Sentiment Words and Vectorize News Data
Complete 4_identify_and_vectorize.py.

(a) Identify top 10 sentiment-bearing words
Choose words such as gain, loss, strong, weak, upgrade, downgrade, etc.
Justify each choice and explain how they are used in vectorization.

(b) Aggregate news
Aggregate all news over three trading days and save as aggregated_news.csv:
aggregated_news(date, symbol, news)

(c) Preprocess text
Apply:
- Tokenization, lowercasing, punctuation removal
- Extract main content only
- Remove HTML tags, ads, navigation menus
- Handle stop words, negations, special characters
- (Optional) stemming or lemmatization

Briefly describe and justify preprocessing choices.

(d) Vectorize
Use:
- Document-Term Matrix (DTM)
- TF-IDF weighted DTM
- Custom (curated) feature matrix

Each with the schema:
(date, symbol, news_vector, impact_score)

Phase 2: Text Classification for Sentiment Analysis

2.1 Data Preparation
Complete 1_process.py.

Split each vectorized dataset into training (80%) and testing (20%) sets over a consecutive three-year segment, preserving time order.

2.2 Classification Modeling

(a) MLP Design
Complete 2_model.py.
Design an MLP in PyTorch. Document:
- Number of layers, neurons per layer, activation functions
- Learning rate, batch size, optimizer, epochs

(b) Training
Complete 2_training.py.
Train and save as model_name.pth using torch.save().

(c) Evaluation
Complete 4_eval.py.
Load your model and evaluate % accuracy. Discuss results in your report.

Phase 3: Trading Simulation

3.1 Trading Rules
Complete 1_trading_rules.py.

Assumptions:
- All orders executed at daily close price.
- No transaction fees.

Trading strategies:
- Use predicted impact scores (from model).

Buy rule:
x = max[1, floor((α * s% * b) / p)], if p ≥ 1
where p = price, s = impact score, b = cash balance, α ≤ 10

Sell rule:
If news is negative and you own shares:
y =
r - x, if r ≥ x
r, if r < x
where r = shares owned.

Neutral rule: take no action.

3.2 Trading Simulation and Evaluation
Complete 2_trading_sim_eval.py.

(a) Setup and liquidation
- Start with $100,000
- Buy/Sell at close prices
- Sell all stocks on final trading day

(b) Track and log
For each trade, calculate:
- $ gain/loss
- total gain/loss
- annual % return
- total % return
- available cash

Log to:
trade_log.csv → transaction details
final_summary.csv → summary totals

Phase 4: Analysis and Reflection

4.1 Summary and Conclusion
Summarize your results, discuss effectiveness, limitations, and biases.

4.2 Improvement and Future Work
Suggest improvements such as:
- Better feature engineering
- Advanced NLP models
- Alternative financial metrics
- Improved market-adjustment methods
Optionally critique flaws in the impact scoring or trading strategy.

Deliverables
- One report file (PDF/Word) including:
  - All team members and % contribution
  - All answers and results
  - Only representative data samples

- All Python program files and generated CSVs
Do not upload original news datasets or Docker images.
If total size > 1 GB, contact Tenshi (the grader).

Grading Criteria
- Quality and correctness of implementation
- Understanding demonstrated in report
- Effort (10%)

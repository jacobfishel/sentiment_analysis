Assignment #2: Sentiment Analysis
Total score: 130
This assignment can be completed individually or by a team of a maximum of three members.
Due date: See the course page

Objectives

Build sentiment analysis models and apply them to financial news to estimate the impact of news
on stock prices.

Datasets
Two financial news datasets collected for several years (2009-2020) by publishers are provided:
â€¢

â€œanalyst_ratings.csvâ€ â€“ Headlines collected by Publisher A, which may include the price
targets estimated by professional analysts through complex modeling to estimate the
current intrinsic value of the company.

â€¢

â€œheadlines.csvâ€ â€“ Headlines collected by Publisher B, without price target information.

Both datasets follow a similar schema:
analyst_ratings(id, headline, URL, publisher, date, symbol)
â€¢ id: row number
â€¢ headline: headline news that may include the target price (not all headlines indicate the
target price).
â€¢ URL: the URL of the headline news
â€¢ Publisher: news publisher
â€¢ Date: published date and time
â€¢ stock: stock symbol representing the company used by traders in the market.
headlines(id, headline, URL, publisher, date, symbol)

Problem Context
Stock price fluctuates daily due to company-specific developments, sector-wide trends, market
sentiments, and/or unpredictable external factors. Since a company's intrinsic value is unknown,
predicting stock price movements is extremely challenging. In this assignment, you will develop
sentiment analysis models that can estimate the potential impact of financial news on future
stock prices and write a brief analysis report about the modeling results.

1

Organization of Program Files and Directories
The required program files and directory structure are provided in â€œAssignment_2.zipâ€ for
efficient grading. Before getting started, review the helper functions and test file: â€œutil.pyâ€,
â€œwebscraping.pyâ€, â€œwebscraping_headless_parsing.pyâ€, â€œdataset_schema.pyâ€,
â€œtest_dataset_schema.pyâ€. These files are provided to help you standardize and validate your
CSV files. Each file in the directories is currently blank and must be completed by you. The file
names correspond to the required tasks described in the sections below. Do not change any file
names, directory names, or the directory structure. For example, the program
â€œ2_trading_sim_eval.pyâ€ should be executable with the command:
â€œpython 2_trading_sim_eval.pyâ€ without requiring additional parameters or instructions.

Required Tasks
Phase 1: Data Collection and Preprocessing
1.1 Collect historical prices, S&P 500 indices, and news data
Complete the program â€œ1_collect_data.pyâ€ for the following task.
(a) Download historical daily prices for each stock symbol in the datasets, along with S&P
500 index for the same period, from a freely available data sources (e.g., Yahoo Finance)
using any available Python package (e.g., â€œyfinanceâ€ or â€œyahoo-finâ€). Save the price data
locally in a file named â€œhistorical_prices.csvâ€. For each symbol, the required fields are at
least: â€œsymbolâ€, â€œdateâ€, â€œopenâ€, â€œhighâ€, â€œlowâ€, â€œAdjCloseâ€, and â€œvolumeâ€. Note that
adjusted close provides a more accurate price than the raw close price. So, we will use it
for close price. For consistency, treat the daily S&P 500 index values like stock prices
with the symbol â€œs&pâ€, and include them in historical_prices.csv. You may include
additional

fields

if

needed

for

more

detailed

analysis.

The

schema

for

historical_prices.csv will be:
historical_prices(date, symbol, open, high, low, close, volume)
(b) Download the full news articles from the URLs provided in the dataset for at least six
consecutive years. These articles will be used to create training and test datasets. You

2

may use Python web scrapping tools (e.g., requests, BeautifulSoup, playwright, selenium,
etc.).
Note: When scraping, please follow ethical and legal guidelines
â€¢

Check each websiteâ€™s robots.txt file to ensure scraping is allowed.

â€¢

Respect rate limits to avoid overloading servers.

â€¢

Use official APIs where available, since they may provide more reliable and
compliant access to the data.

Full new articles can be downloaded from several sites, including Benzinga, Zacks,
Gurufocus, and Seekingalpha. However, Gurufocus appears to block any attempts at
automated scrapping. Refer to the sample code for web scraping.
If none of the sites allowed scraping, this step will be skipped.
(c) Create a new dataset â€œall_news.csvâ€ by merging the two datasets. Include all fields from
the original datasets, removing redundant content if any, and add a new field â€œarticleâ€ for
the full news articles. The schema for â€œall_news.csvâ€ will be:
all_news(date, symbol, headline, URL, article, publisher)
1.2 Calculate volatility
Complete the program â€œ2_calculate_volatility.pyâ€ for the following task.
We assume that impactful news affects the stock price for about three days. The average
daily volatility (implied daily risk) for each company and the S&P 500 index is calculated as
the standard deviation of returns over three trading days, expressed as a percentage.
â€¢

Daily log return
ğ‘ƒğ‘ƒ âˆ’ğ‘ƒğ‘ƒ

ğ‘ƒğ‘ƒ

From the simple return in percentage ğ‘…ğ‘…ğ‘¡ğ‘¡ = ğ‘¡ğ‘¡ğ‘ƒğ‘ƒ ğ‘¡ğ‘¡âˆ’1 = ğ‘ƒğ‘ƒ ğ‘¡ğ‘¡ âˆ’ 1 and continuous
ğ‘¡ğ‘¡âˆ’1

ğ‘¡ğ‘¡âˆ’1

compounding formula ğ‘ƒğ‘ƒğ‘¡ğ‘¡ = ğ‘ƒğ‘ƒğ‘¡ğ‘¡âˆ’1 ğ‘’ğ‘’ ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡ , we can calculate the daily log returns:
ğ‘ƒğ‘ƒ

ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡ = ln(ğ‘ƒğ‘ƒ ğ‘¡ğ‘¡ ) or ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡ = ln(1 + ğ‘…ğ‘…ğ‘¡ğ‘¡ ) where ğ‘ƒğ‘ƒğ‘¡ğ‘¡ is the price at time t and ğ‘ƒğ‘ƒğ‘¡ğ‘¡âˆ’1 is the price at
ğ‘¡ğ‘¡âˆ’1

previous time (three trading days ago).

3

â€¢

Daily volatility
The standard deviation of returns provides the average daily volatility: ğœğœğ‘¤ğ‘¤ = stdev(ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡ )

for each stock and S&P 500 index. Optionally, periodic volatility can be calculated

for longer intervals: ğœğœğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = ğœğœğ‘¤ğ‘¤ Ã— âˆšğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡ğ‘¡. For example, the annualized volatility:

ğœğœğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ = ğœğœğ‘¤ğ‘¤ Ã—âˆš252 (assuming 252 trading days in a year). Note: Volatility is the
square root of variance and can be interpreted as risk. Risks accumulate additively in
variance, so annual volatility can be considered as yearly risk.

â€¢

Market-adjusted return and volatility
Stock prices are influenced by both market sentiment (reflected in S&P 500 volatility)
and company-specific news. To account for market influence, we can calculate the
market-adjusted return (also called abnormal return) and market-adjusted volatility by
regressing the stockâ€™s daily log returns on the marketâ€™s daily log returns.
The market model is defined as: ğ’“ğ’“ğ’‚ğ’‚,ğ’•ğ’• = ğœ¶ğœ¶ + ğœ·ğœ·ğ’“ğ’“ğ’ğ’,ğ’•ğ’• + ğğğ’•ğ’•
where:
o
o
o

ğ‘Ÿğ‘Ÿğ‘ğ‘,ğ‘¡ğ‘¡ = market model, observed asset ğ‘ğ‘ return at time t (that is ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡ )
ğ‘Ÿğ‘Ÿğ‘šğ‘š,ğ‘¡ğ‘¡ = market (S&P 500) log return at time t

ğ›¼ğ›¼ï¿½ = ğ‘Ÿğ‘Ÿï¿½ğ‘¡ğ‘¡ âˆ’ ğ›½ğ›½Ì‚ ğ‘Ÿğ‘Ÿï¿½ï¿½ï¿½ï¿½ï¿½
ğ‘šğ‘š,ğ‘¡ğ‘¡ (Intercept, expected return after accounting for market exposure)
ğ‘ğ‘

âˆ‘ (ğ‘Ÿğ‘Ÿ âˆ’ğ‘Ÿğ‘Ÿï¿½ )(ğ‘Ÿğ‘Ÿğ‘šğ‘š,ğ‘¡ğ‘¡ âˆ’ğ‘Ÿğ‘Ÿ
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)
ğ¶ğ¶ğ¶ğ¶ğ¶ğ¶(ğ‘Ÿğ‘Ÿ ,ğ‘Ÿğ‘Ÿ )
ğ‘šğ‘š,ğ‘¡ğ‘¡
o ğ›½ğ›½Ì‚ = ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰ğ‘‰(ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡ ğ‘šğ‘š,ğ‘¡ğ‘¡) = ğ‘¡ğ‘¡ âˆ‘ğ‘ğ‘ğ‘¡ğ‘¡ (ğ‘Ÿğ‘Ÿ ğ‘¡ğ‘¡ âˆ’ğ‘Ÿğ‘Ÿ
(stockâ€™s sensitivity to the market)
ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½)2
ğ‘šğ‘š,ğ‘¡ğ‘¡

ğ‘¡ğ‘¡

ğ‘šğ‘š,ğ‘¡ğ‘¡

ğ‘šğ‘š,ğ‘¡ğ‘¡

o ğœ–ğœ–ğ‘¡ğ‘¡ = idiosyncratic return (residual)

The market-adjusted return (or idiosyncratic return after removing the market
ï¿½ ğ’“ğ’“ğ’ğ’,ğ’•ğ’• )
ï¿½ + ğœ·ğœ·
effect) is: ğğï¿½ğ’•ğ’• = ğ’“ğ’“ğ’‚ğ’‚,ğ’•ğ’• âˆ’ (ğœ¶ğœ¶
The market-adjusted volatility (or idiosyncratic volatility) is:
ğŸğŸ

ï¿½)ğŸğŸ
ğˆğˆğœºğœºğ’•ğ’• = ğ¬ğ¬ğ¬ğ¬ğ¬ğ¬ğ¬ğ¬ğ¬ğ¬(ğœºğœºğ’•ğ’• ) = ï¿½ğ‘µğ‘µâˆ’ğŸğŸ âˆ‘ğ‘µğ‘µ
ğ’•ğ’• (ğœºğœºğ’•ğ’• âˆ’ ğœºğœº
The asset volatility (combining market-driven and idiosyncratic components) is:
ğˆğˆğ’“ğ’“ğ’‚ğ’‚,ğ’•ğ’• = ğ¬ğ¬ğ¬ğ¬ğ¬ğ¬ğ¬ğ¬ğ¬ğ¬(ğ’“ğ’“ğ’‚ğ’‚,ğ’•ğ’• ) where ğ’“ğ’“ğ’‚ğ’‚,ğ’•ğ’• = ğœ¶ğœ¶ + ğœ·ğœ·ğ’“ğ’“ğ’ğ’,ğ’•ğ’• + ğğğ’•ğ’•

4

â€¢

Interpretation
Beta (ğœ·ğœ·) measures how sensitive your stock is to the market, capturing the systemic
risk (the part of stock volatility explained by market movements. So, ğ›½ğ›½ = 1, stock
moves in line with the market; ğ›½ğ›½ > 1, the stock is more volatile than the market; ğ›½ğ›½ <

1, stock is less volatile than the market; ğ›½ğ›½ < 0, the stock moves opposite to the
market.

Alpha (ğœ¶ğœ¶) represents abnormal/excess return, that is, the expected return of the stock
after accounting for its exposure to the market. So, ğ›¼ğ›¼ > 0, stock outperforms whatâ€™s
expected given its beta; ğ›¼ğ›¼ < 0, stock underperforms whatâ€™s expected given its beta.

Residual (ğğğ’•ğ’• ) is the idiosyncratic return, the portion of the stock return not explained

by the market.

1.3 Estimate impact scores
Complete the program â€œ3_estimate_impact_scores.pyâ€ for the following task.
(a) Calculate the impact score based on the return and volatility, and map it into a discrete
range [-3, +3] as impact scores, where -1, -2, -3 = negative impact, 1, 2, 3 = positive
impact, and 0 = neutral, formally: ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–_ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘¡ğ‘¡ = ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ (ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡ ) âˆ™ ğ‘“ğ‘“(ğœğœğ‘¡ğ‘¡ ).

â€¢
â€¢

Return ğ’“ğ’“ğ’•ğ’• captures the direction of the market reaction. For computing the impact
score, we will use the market-adjusted return ğğğ’•ğ’• instead of the daily return ğ‘Ÿğ‘Ÿğ‘¡ğ‘¡ .

Volatility ğˆğˆğ’•ğ’• captures the magnitude/uncertainty of the market reaction. We will use
the total stock volatility ğˆğˆğ’“ğ’“ğ’‚ğ’‚,ğ’•ğ’• (including both market-driven and idiosyncratic

â€¢

components) instead of daily volatility ğœğœğ‘¡ğ‘¡ .

Implication of the impact score: A large impact score indicates a large positive
return with high volatility (+3), a small impact score indicates a small positive return
with low volatility (+1), a large negative impact score indicates a large negative
return with high volatility (âˆ’3), and other combinations follow similarly.

Z-score normalization and composite score
â€¢

Normalize both return and volatility using z-scores:

5

ğğ âˆ’ğœ‡ğœ‡

â€¢

Normalized return ğ‘§ğ‘§ğ‘Ÿğ‘Ÿ = ğ’•ğ’•ğœğœ ğ‘Ÿğ‘Ÿ and volatility ğ‘§ğ‘§ğœğœ =
ğ‘Ÿğ‘Ÿ

ğˆğˆğœºğœºğ’•ğ’• âˆ’ğœ‡ğœ‡ğœğœ
ğœğœğœğœ

Define a composite impact score function that maps return and volatility into impact
scores:
ğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Šğ’Š_ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”ğ’•ğ’• = ï¿½
where:

ğ¢ğ¢ğ¢ğ¢ |ğ’›ğ’›ğ’“ğ’“ | â‰¤ ğŸğŸ. ğŸ“ğŸ“
ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”ğ’”(ğ’›ğ’›ğ’“ğ’“ ) âˆ™ ï¿½ğŸğŸ + ğ‘°ğ‘°(|ğ’›ğ’›ğ’“ğ’“ | > ğŸğŸ) + ğ‘°ğ‘°(ğ’›ğ’›ğˆğˆ > ğŸğŸ)ï¿½, ğ¢ğ¢ğ¢ğ¢ |ğ’›ğ’›ğ’“ğ’“ | > ğŸğŸ. ğŸ“ğŸ“
ğŸğŸ,

o ğ¼ğ¼(âˆ™) is an indicator function that equals 1 if the condition is true, 0 otherwise.

o Return thresholds: Large move if |ğ‘§ğ‘§ğ‘Ÿğ‘Ÿ | > 1, small move if 0.5 < |ğ‘§ğ‘§ğ‘Ÿğ‘Ÿ | â‰¤ 1 ,
neutral: |ğ‘§ğ‘§ğ‘Ÿğ‘Ÿ | â‰¤ 0.5

o Volatility thresholds: ğ‘§ğ‘§ğœğœ > 1, normal volatility otherwise

(b) Create a new dataset â€œhistorical_prices_impact.csvâ€ from historical_prices.csvâ€ by
adding the returns, volatilities and impact score calculated from sections 1.2 and 1.3.
The schema for historical_prices_impact.csv will be:
historical_prices_impact(date, symbol, open, high, low, close, volume, daily_return,
daily_volatility, market_return, beta, alpha, market_adj_return, market_adj_volatility,
impact_score)
1.4 Identify sentiment words and vectorize the news data
Complete the program â€œ4_identify_and_vectorize.pyâ€ for the following task.
(a) From the datasets, identify the top 10 sentiment-bearing words that are generally
known to influence stock prices (e.g., gain, loss, strong, weak, upgrade, downgrade, etc.),
list them with a brief justification of your choices based on the supporting evidence, and
explain how these words were incorporated into the vectorization process.
(b) Create a new dataset â€œaggregated_news.csvâ€ from â€œall_news.csvâ€ by aggregating all the
news (both headlines and full articles) over three consecutive trading days. This
aggregation assumes that stock prices are influenced by news from the past three trading
days. Specifically, we assume that the current impact score at time ğ‘¡ğ‘¡ reflects news

published at time ğ‘¡ğ‘¡ as well as the previous two trading days (ğ‘¡ğ‘¡, ğ‘¡ğ‘¡ âˆ’ 1, ğ‘¡ğ‘¡ âˆ’ 2). The schema

for â€œaggregated_news.csvâ€ will be:

6

aggregated_news(date, symbol, news) where news is aggregated news.
(c) Preprocess the news data for sentiment analysis.
The headlines or full news articles are in text or HTML format and can contain
unnecessary text content. As discussed in class, the possible text preprocessing tasks are:
â€¢

Tokenization and text normalization: extracting tokens, lowercasing, removing
punctuation, etc.

â€¢

Relevant text extraction: to ensure that only the main content of the news article or
headline is retained, removing irrelevant contents such as HTML tags, advertisement,
navigation menus, or unrelated HTML content.

â€¢

Additional text processing: to handle stop words, negation, occurrence of important
words, etc. (see slide #29).

â€¢

Optional preprocessing steps: stemming or lemmatization, or handling special
characters.

After preprocessing, briefly describe the preprocessing methods you applied and explain
why they are important for sentiment analysis.
(d) Vectorize the aggregated news in â€œaggregated_news.csvâ€ using a Document-Term Matrix
(DTM), a TF-IDF weighted DTM, and a custom (curated) feature matrix (see slide #43
for an example) and create new datasets by adding the impact score from
â€œhistorical_prices_impact.csvâ€. The schema for each dataset will be:
vectorized_news_dtm(date, symbol, news_vector, impact_score)
vectorized_news_tfidf(date, symbol, news_vector, impact_score)
vectorized_news_curated(date, symbol, news_vector, impact_score)

7

Phase 2: Text Classification for Sentiment Analysis
2.1 Data preparation
Complete the program â€œ1_process.pyâ€ for the following task.
Dataset selection and splitting for each vectorized dataset in section 1.4 (d)
â€¢

Select a consecutive time segment of at least three years and split it into training (80%)
and testing (20%) sets, maintaining sequential order within each segment. The selected
segment may come from the beginning, middle, end, or all of the dataset, but the split
must preserve sequential time order to avoid mixing future data into the past or
prevent lookahead bias.

2.2 Classification modeling
(a) MLP design for training
Complete the program â€œ2_model.pyâ€ for the following task.
Design a MLP in Pytorch to train a classifier on each vectorized dataset created in
section 1.4 (d) and clearly describe the key architectural choices, including:
â€¢

Number of layers, Number of neurons per layer, Activation functions

â€¢

Other hyperparameters (e.g., learning rate, batch size, optimizer, number of epochs).

(b) Training
Complete the program â€œ2_training.pyâ€ for the following task.
Train the model and save the model as â€œ[name of your model].pthâ€. You can do this
through torch.save(). For further information, refer to PyTorch.
(c) Evaluation
Complete the program â€œ4_eval.pyâ€ for the following task.
Load the model you saved in step (b). Evaluate and compare the performance of each
classifier in terms of % accuracy and discuss the results in the report file.

Phase 3: Trading Simulation
3.1 Trading rules
Complete the program â€œ1_trading_rules.pyâ€ for the following task.
8

Assumption:
â€¢

All orders are executed at the daily closing price.

â€¢

No transaction fees are considered.

Impact score estimation and trading strategies
For each day, estimate the impact score for the past three trading days of news using the
trained model. For trading decisions, use the impact scores predicted by the model
(learned during training) rather than the precalculated impact scores from Section 1.3.
Implement a simple sentiment-driven trading system based on the following strategies:
Buy rule: If news has a positive score, buy x number of shares:
ğ’™ğ’™ = ğ¦ğ¦ğ¦ğ¦ğ¦ğ¦ [ğŸğŸ, ğ’‡ğ’‡ğ’‡ğ’‡ğ’‡ğ’‡ğ’‡ğ’‡ğ’‡ğ’‡(

where:
â€¢
â€¢

ğœ¶ğœ¶âˆ™ğ’”ğ’”%âˆ™ğ’ƒğ’ƒ
ğ’‘ğ’‘

ğ’ƒğ’ƒ

)] if ğ’‘ğ’‘ â‰¥ ğŸğŸ, 0 otherwise

p = stock price, ğ‘ ğ‘  = impact score, ğ‘ğ‘ = current cash balance, ğ›¼ğ›¼ = a multiplier â‰¤ 10
(e.g., ğ›¼ğ›¼ = 2, used to determine the amount of investment for each order)

ğœ¶ğœ¶ âˆ™ ğ’”ğ’”% = ğ›¼ğ›¼ Ã— ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–_ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ ğ‘ % (e.g., if ğ›¼ğ›¼ = 2 and ğ‘ ğ‘  = 2, then 2ğ‘ ğ‘ % = 4%).

Buy only if the balance is sufficient enough to purchase at least one share. For example,
the current balance ğ‘ğ‘ = $20,000, stock price p = $10.3,

and impact score ğ‘ ğ‘  = 2, then ğ‘¥ğ‘¥ = ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“ğ‘“(

0.04Ã—20000
10.3

) = 77. So, you buy 77 shares.

Sell rule: If news has a negative score and you own at least one share (no short selling
ğ’“ğ’“ âˆ’ ğ’™ğ’™, ğ¢ğ¢ğ¢ğ¢ ğ’“ğ’“ â‰¥ ğ’™ğ’™
allowed), sell y number of shares: ğ’šğ’š = ï¿½
.
ğ’“ğ’“, ğ¢ğ¢ğ¢ğ¢ ğ’“ğ’“ < ğ’™ğ’™
where:
â€¢

â€¢

ğ‘Ÿğ‘Ÿ > 0 = number of shares currently owned

ğ‘¥ğ‘¥ = number of shares calculated from the buy rule above

Neural rule: If new is neutral, take no action (no buy or sell).
3.2 Trading simulation for performance evaluation

Complete the program â€œ2_trading_sim_eval.pyâ€ for the following task.

9

(a) Initial setup and liquidation
â€¢

Assume your account starts with an initial balance of $100,000 on the first trading
day in the dataset. All Buy/Sell orders are assumed to be filled immediately at the
requested close price with no trading fees.

â€¢

On the final trading day in the dataset, sell all remaining stocks at the closing
price, ensuring that the account balance is fully in cash.

(b) Trade tracking and performance metrics
For each trade, calculate:
â€¢

$gain/loss for the trade

â€¢

Total $gain/loss for all trades

â€¢

Annual % return,

â€¢

Total % return relative to the initial balance

â€¢

Available cash for trade

Log all trade data into a file â€œtrade_log.csvâ€, including the following fields:
â€¢

Transaction date

â€¢

Symbol

â€¢

Trade type (Buy/Sell)

â€¢

Number of shares

â€¢

Price

â€¢

Transaction amount

â€¢

Available cash after the trade

â€¢

News (headline)

â€¢

Impact score

Log the final summary into a file â€œfinal_summary.csvâ€, including the following fields:
â€¢

Total $gain/loss

â€¢

Average annual % return

â€¢

Total % return

â€¢

Final account balance

10

Note: The log file serves as a valuable resource for debugging and validating your trading
simulation results.

Phase 4: Analysis and Reflection
4.1 Summary and conclusion
(a) Briefly summarize your overall sentiment analysis results, highlighting key observations.
Evaluate the effectiveness of trading strategies informed by the return and volatilitybased impact scoring method and discuss any limitations of the method, including
assumptions made, potential biases, or data constraints.
(b) Reflect on your experience, including challenges faced, insights gained, and lessons
learned from implementing the workflow.
4.2 Improvement and future work
(a) Suggest any potential improvements or alternative approaches that could achieve more
accurate sentiment analysis in future studies, e.g., better feature engineering, more
sophisticated NLP models, alternative financial metrics, or improved market-adjustment
methods, etc.
(b) Optionally, identify any logical or mathematical flaws in the impact score method and
trading strategies implemented in this assignment. Discuss how these could be addressed
or mitigated in future work.

Deliverables
â€¢

One report file (PDF or Word) that includes:
o All team member names and % contribution of each member. If every member
contributed equally, simply state "equal contribution." If there is disagreement on
contributions, provide a brief task description for each member. In this case, different
grades may be assigned individually.
o All answers and results from the required tasks
o When including example data in your report, show only a few representative samples
instead of the entire dataset.

11

â€¢

All Python program files you created, and CSV files generated from your programs. DO
not upload the original news datasets or Docker images. Please contact Tenshi (the grader) if
the total size exceeds 1 GB. In that case, include only a portion of your datasets.

Grading Criteria
â€¢

Quality of work shown on the report (e.g., modeling process and results, methods used, and
correctness of implementation and analysis)

â€¢

Level of understanding of related subjects as reflected in the report

â€¢

Effort (10%)

12

